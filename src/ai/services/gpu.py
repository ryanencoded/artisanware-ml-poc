def generate_with_gpu(prompt: str, model_name: str, max_new_tokens: int | None = None) -> str:
    # Placeholder: connect to vLLM / TGI or local GPU model
    return f"[GPU] Not yet implemented for {model_name}. Prompt: {prompt}"
